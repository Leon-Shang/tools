{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity and manifold regularized convolutional auto-encoders-based feature learning for fault detection of multivariate processes\n",
      "54\n",
      "new51\n",
      "Convolutional Long Short-Term Memory Autoencoder-Based Feature Learning for Fault Detection in Industrial Processes\n",
      "74\n",
      "new67\n",
      "Manifold regularized stacked autoencoders-based feature learning for fault detection in industrial processes\n",
      "74\n",
      "new70\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "def might_be_title(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Heuristic rules\n",
    "    num_tokens = len(doc)\n",
    "    # num_proper_nouns = sum(token.pos_ == \"PROPN\" for token in doc)\n",
    "\n",
    "    # num_nouns = sum(token.pos_ == \"NOUN\" for token in doc)\n",
    "    # num_verbs = sum(token.pos_ == \"VERB\" for token in doc)\n",
    "    # Check if the text contains a minimum number of proper nouns or nouns, and fewer verbs\n",
    "    if (num_tokens >= 4):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def parse_references_regex(ref_list):\n",
    "    pattern = r'\\(\\d{4}\\)'\n",
    "\n",
    "    titles = re.findall(pattern, ref_list)\n",
    "    refs = [re.findall(r'(.*)\\(\\d{4}\\)', i)[0]  for i in ref_list.split(\",\") if (len(re.findall(pattern, i))>=1)]\n",
    "    return refs\n",
    "\n",
    "input_csv = 'input.csv'\n",
    "output_csv = 'output.csv'\n",
    "\n",
    "# Read input CSV\n",
    "with open(input_csv, 'r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    paper_titles = []\n",
    "    reference_lists = []\n",
    "\n",
    "    for row in reader:\n",
    "        paper_title, ref_list_str = row\n",
    "        paper_titles.append(paper_title)\n",
    "        reference_lists.append(parse_references_regex(ref_list_str))\n",
    "new_reference_lists = []\n",
    "for paper_title, ref_title in zip(paper_titles, reference_lists):\n",
    "    print(paper_title)\n",
    "    print(len(ref_title))\n",
    "    new_ref_titles = []\n",
    "    for ref in ref_title:\n",
    "        if might_be_title(ref):\n",
    "            new_ref_titles.append(ref)\n",
    "    print(f'new{len(new_ref_titles)}')\n",
    "    new_reference_lists.append(new_ref_titles)\n",
    "# Write output CSV\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # Write paper titles as the first row\n",
    "    writer.writerow(paper_titles)\n",
    "\n",
    "    # Write reference titles row by row\n",
    "    max_refs = max(len(ref_list) for ref_list in reference_lists)\n",
    "    for row_idx in range(max_refs):\n",
    "        row = []\n",
    "        for ref_list in reference_lists:\n",
    "            if row_idx < len(ref_list):\n",
    "                row.append(ref_list[row_idx])\n",
    "            else:\n",
    "                row.append(\"\")  # Empty cell if no more references in the list\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers found in all three columns:\n",
      " Imagenet classification with deep convolutional neural networks \n",
      " A comparison study of basic data-driven fault diagnosis and process monitoring methods on the benchmark Tennessee Eastman process \n",
      " Deep convolutional neural network model based chemical process fault diagnosis \n",
      " \n",
      " A convolutional neural network for fault classification and diagnosis in semiconductor manufacturing processes \n",
      " Process monitoring through manifold regularization-based GMM with global/local information \n",
      " Kernel density estimation for an anomaly based intrusion detection system \n",
      " Monitoring and diagnosing of mean shifts in multivariate manufacturing processes using two-level selective ensemble of learning vector quantization neural networks \n",
      " A plant-wide industrial process control problem \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV data\n",
    "data = \"\"\"\n",
    "paper_1,paper_2,paper_3\n",
    "Title A,Title B,Title A\n",
    "Title B,Title C,Title C\n",
    "Title C,Title A,Title B\n",
    "\"\"\"\n",
    "\n",
    "# Create a DataFrame from the CSV data\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# Find the intersection of the paper titles in all three columns\n",
    "common_papers = set(df.iloc[:,0])\n",
    "for col in range(1, df.shape[1]):  # Iterate through the remaining columns\n",
    "    common_papers.intersection_update(df.iloc[:, col])\n",
    "# Print the common papers\n",
    "print(\"Papers found in all three columns:\")\n",
    "for paper in common_papers:\n",
    "    print(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = \"The Catcher in the Rye\"\n",
    "print(might_be_title(text))  # True\n",
    "\n",
    "text = \"dog and cat playing together\"\n",
    "print(might_be_title(text))  # False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E001] No component 'tokenizer' found in pipeline. Available names: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[39mreturn\u001b[39;00m Doc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab, words\u001b[39m=\u001b[39mwords, spaces\u001b[39m=\u001b[39mspaces)\n\u001b[0;32m     27\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mblank(\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m nlp\u001b[39m.\u001b[39;49mremove_pipe(\u001b[39m\"\u001b[39;49m\u001b[39mtokenizer\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m# Remove the default tokenizer\u001b[39;00m\n\u001b[0;32m     29\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(\u001b[39m\"\u001b[39m\u001b[39msentence_tokenizer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThis is a sentence. And here is another one. Let\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms tokenize them into sentences.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\installed_software\\anaconda\\envs\\py39_tools\\lib\\site-packages\\spacy\\language.py:950\u001b[0m, in \u001b[0;36mLanguage.remove_pipe\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Remove a component from the pipeline.\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \n\u001b[0;32m    944\u001b[0m \u001b[39mname (str): Name of the component to remove.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[39mDOCS: https://spacy.io/api/language#remove_pipe\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    949\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponent_names:\n\u001b[1;32m--> 950\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE001\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, opts\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponent_names))\n\u001b[0;32m    951\u001b[0m removed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_components\u001b[39m.\u001b[39mpop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponent_names\u001b[39m.\u001b[39mindex(name))\n\u001b[0;32m    952\u001b[0m \u001b[39m# We're only removing the component itself from the metas/configs here\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# because factory may be used for something else\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: [E001] No component 'tokenizer' found in pipeline. Available names: []"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.factory(\"sentence_tokenizer\")\n",
    "def create_sentence_tokenizer(nlp, name):\n",
    "    return SentenceTokenizer(nlp)\n",
    "\n",
    "class SentenceTokenizer:\n",
    "    def __init__(self, nlp):\n",
    "        self.vocab = nlp.vocab\n",
    "        self.sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "\n",
    "    def __call__(self, text):\n",
    "        doc = nlp.make_doc(text)\n",
    "        doc = self.sentencizer(doc)\n",
    "        sentence_tokens = [sent.text for sent in doc.sents]\n",
    "        words = []\n",
    "        spaces = []\n",
    "\n",
    "        for sent in sentence_tokens:\n",
    "            words.extend(sent.split())\n",
    "            spaces.extend([True] * (len(sent.split()) - 1) + [False])\n",
    "\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.remove_pipe(\"tokenizer\")  # Remove the default tokenizer\n",
    "nlp.add_pipe(\"sentence_tokenizer\")\n",
    "\n",
    "text = \"This is a sentence. And here is another one. Let's tokenize them into sentences.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abdeljaber\n",
      "Abdeljaber,\n",
      ",\n",
      ", O.\n",
      ", O.,\n",
      "O.\n",
      "O.,\n",
      ",\n",
      ", Avci\n",
      ", Avci,\n",
      "Avci\n",
      "Avci,\n",
      ",\n",
      ", O.\n",
      ", O.,\n",
      "O.\n",
      "O.,\n",
      ",\n",
      ", Kiranyaz\n",
      ", Kiranyaz,\n",
      "Kiranyaz\n",
      "Kiranyaz,\n",
      ",\n",
      ", M.S.\n",
      ", M.S.,\n",
      "M.S.\n",
      "M.S.,\n",
      ",\n",
      ", Boashash\n",
      ", Boashash,\n",
      "Boashash\n",
      "Boashash,\n",
      ",\n",
      ", B.\n",
      ", B.,\n",
      "B.\n",
      "B.,\n",
      ",\n",
      ", Sodano\n",
      ", Sodano,\n",
      "Sodano\n",
      "Sodano,\n",
      ",\n",
      ", H.\n",
      ", H.,\n",
      "H.\n",
      "H.,\n",
      ",\n",
      ", Inman\n",
      ", Inman,\n",
      "Inman\n",
      "Inman,\n",
      ",\n",
      ", D.J.\n",
      ", D.J.,\n",
      "D.J.\n",
      "D.J.,\n",
      ",\n",
      ", D.J., 1\n",
      ", D.J., 1-\n",
      "D.J., 1\n",
      "D.J., 1-\n",
      ", 1\n",
      ", 1-\n",
      "1\n",
      "1-\n",
      "-\n",
      ":\n",
      "2018\n",
      "2018)\n",
      ")\n",
      ") Neurocomputing\n",
      ") Neurocomputing,\n",
      "Neurocomputing\n",
      "Neurocomputing,\n",
      ",\n",
      ") Neurocomputing, 275\n",
      ") Neurocomputing, 275,\n",
      "Neurocomputing, 275\n",
      "Neurocomputing, 275,\n",
      ", 275\n",
      ", 275,\n",
      "275\n",
      "275,\n",
      ",\n",
      ", pp\n",
      ", pp.\n",
      "pp\n",
      "pp.\n",
      ".\n",
      ", pp. 1308\n",
      ", pp. 1308-\n",
      "pp. 1308\n",
      "pp. 1308-\n",
      ". 1308\n",
      ". 1308-\n",
      "1308\n",
      "1308-\n",
      "-\n",
      "-1317\n",
      "-1317;\n",
      "1317\n",
      "1317;\n",
      ";\n",
      "; Arunthavanathan\n",
      "; Arunthavanathan,\n",
      "Arunthavanathan\n",
      "Arunthavanathan,\n",
      ",\n",
      ", R.\n",
      ", R.,\n",
      "R.\n",
      "R.,\n",
      ",\n",
      ", Khan\n",
      ", Khan,\n",
      "Khan\n",
      "Khan,\n",
      ",\n",
      ", F.\n",
      ", F.,\n",
      "F.\n",
      "F.,\n",
      ",\n",
      ", Ahmed\n",
      ", Ahmed,\n",
      "Ahmed\n",
      "Ahmed,\n",
      ",\n",
      ", S.\n",
      ", S.,\n",
      "S.\n",
      "S.,\n",
      ",\n",
      ", Imtiaz\n",
      ", Imtiaz,\n",
      "Imtiaz\n",
      "Imtiaz,\n",
      ",\n",
      ", S.\n",
      ", S.,\n",
      "S.\n",
      "S.,\n",
      ",\n",
      ", Rusli\n",
      ", Rusli,\n",
      "Rusli\n",
      "Rusli,\n",
      ",\n",
      ", R.\n",
      ", R.,\n",
      "R.\n",
      "R.,\n",
      ",\n",
      "-\n",
      "2020\n",
      "2020)\n",
      ")\n",
      ") Computers\n",
      ") Computers &\n",
      "Computers\n",
      "Computers &\n",
      "&\n",
      "& Chemical\n",
      "Chemical\n",
      "& Chemical Engineering\n",
      "& Chemical Engineering,\n",
      "Chemical Engineering\n",
      "Chemical Engineering,\n",
      "Engineering\n",
      "Engineering,\n",
      ",\n",
      "& Chemical Engineering, 134\n",
      "& Chemical Engineering, 134;\n",
      "Chemical Engineering, 134\n",
      "Chemical Engineering, 134;\n",
      "Engineering, 134\n",
      "Engineering, 134;\n",
      ", 134\n",
      ", 134;\n",
      "134\n",
      "134;\n",
      ";\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_year_pattern\")\n",
    "def custom_year_pattern(doc):\n",
    "    pattern = r'\\(\\d{4}\\)'\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        doc.char_span(start, end, label=\"YEAR\")\n",
    "    return doc\n",
    "\n",
    "def parse_references_spacy(ref_list):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    nlp.add_pipe(\"custom_year_pattern\", before=\"parser\")\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{\"IS_PUNCT\": True, \"OP\": \"?\"}, {\"POS\": \"PROPN\", \"OP\": \"*\"}, {\"IS_PUNCT\": True, \"OP\": \"?\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}, {\"IS_PUNCT\": True, \"OP\": \"?\"}]\n",
    "    matcher.add(\"TITLE\", [pattern])\n",
    "\n",
    "    doc = nlp(ref_list)\n",
    "    matches = matcher(doc)\n",
    "    titles = []\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        if span.text.startswith(\"(\") :\n",
    "            continue\n",
    "        titles.append(span.text.strip())\n",
    "\n",
    "    return titles\n",
    "\n",
    "ref_list = \"Abdeljaber, O., Avci, O., Kiranyaz, M.S., Boashash, B., Sodano, H., Inman, D.J., 1-D CNNs for structural damage detection: Verification on a structure health monitoring benchmark data (2018) Neurocomputing, 275, pp. 1308-1317; Arunthavanathan, R., Khan, F., Ahmed, S., Imtiaz, S., Rusli, R., Fault detection and diagnosis in process system using artificial intelligence-based cognitive technique (2020) Computers & Chemical Engineering, 134;\"\n",
    "titles = parse_references_spacy(ref_list)\n",
    "\n",
    "for title in titles:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG'), ('San Francisco', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ruler = nlp.add_pipe(\"span_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Apple is opening its first big office in   San Francisco.\")\n",
    "print([(span.text, span.label_) for span in doc.spans[\"ruler\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abdeljaber Abdeljaber PROPN NNP ROOT Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "O. O. PROPN NNP appos X. False False\n",
      ", , PUNCT , punct , False False\n",
      "Avci Avci PROPN NNP nmod Xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "O. O. PROPN NNP appos X. False False\n",
      ", , PUNCT , punct , False False\n",
      "Kiranyaz Kiranyaz PROPN NNP conj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "M.S. M.S. PROPN NNP conj X.X. False False\n",
      ", , PUNCT , punct , False False\n",
      "Boashash Boashash PROPN NNP conj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "B. B. PROPN NNP conj X. False False\n",
      ", , PUNCT , punct , False False\n",
      "Sodano Sodano PROPN NNP conj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "H. H. PROPN NNP appos X. False False\n",
      ", , PUNCT , punct , False False\n",
      "Inman Inman PROPN NNP nmod Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "D.J. D.J. PROPN NNP appos X.X. False False\n",
      ", , PUNCT , punct , False False\n",
      "1 1 NUM CD nummod d False False\n",
      "- - PUNCT HYPH punct - False False\n",
      "D d ADJ JJ compound X True False\n",
      "CNNs cnn NOUN NNS appos XXXx True False\n",
      "for for ADP IN prep xxx True True\n",
      "structural structural ADJ JJ amod xxxx True False\n",
      "damage damage NOUN NN compound xxxx True False\n",
      "detection detection NOUN NN pobj xxxx True False\n",
      ": : PUNCT : punct : False False\n",
      "Verification verification NOUN NN appos Xxxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "a a DET DT det x True True\n",
      "structure structure NOUN NN compound xxxx True False\n",
      "health health NOUN NN compound xxxx True False\n",
      "monitoring monitor VERB VBG nmod xxxx True False\n",
      "benchmark benchmark NOUN NN compound xxxx True False\n",
      "data datum NOUN NNS pobj xxxx True False\n",
      "( ( PUNCT -LRB- punct ( False False\n",
      "2018 2018 NUM CD appos dddd False False\n",
      ") ) PUNCT -RRB- punct ) False False\n",
      "Neurocomputing Neurocomputing PROPN NNP ROOT Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "275 275 NUM CD appos ddd False False\n",
      ", , PUNCT , punct , False False\n",
      "pp pp PROPN NNP appos xx True False\n",
      ". . PROPN NNP appos . False False\n",
      "1308 1308 NUM CD appos dddd False False\n",
      "- - SYM SYM punct - False False\n",
      "1317 1317 NUM CD appos dddd False False\n",
      "; ; PUNCT : punct ; False False\n",
      "Arunthavanathan Arunthavanathan PROPN NNP appos Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "R. R. PROPN NNP appos X. False False\n",
      ", , PUNCT , punct , False False\n",
      "Khan Khan PROPN NNP conj Xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "F. F. PROPN NNP appos X. False False\n",
      ", , PUNCT , punct , False False\n",
      "Ahmed Ahmed PROPN NNP conj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "S. S. PROPN NNP conj X. False False\n",
      ", , PUNCT , punct , False False\n",
      "Imtiaz Imtiaz PROPN NNP conj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "S. S. PROPN NNP conj X. False False\n",
      ", , PUNCT , punct , False False\n",
      "Rusli Rusli PROPN NNP conj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "R. R. PROPN NNP appos X. False False\n",
      ", , PUNCT , punct , False False\n",
      "Fault fault NOUN NN compound Xxxxx True False\n",
      "detection detection NOUN NN conj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "diagnosis diagnosis NOUN NN conj xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "process process NOUN NN compound xxxx True False\n",
      "system system NOUN NN pobj xxxx True False\n",
      "using use VERB VBG acl xxxx True True\n",
      "artificial artificial ADJ JJ amod xxxx True False\n",
      "intelligence intelligence NOUN NN npadvmod xxxx True False\n",
      "- - PUNCT HYPH punct - False False\n",
      "based base VERB VBN amod xxxx True False\n",
      "cognitive cognitive ADJ JJ amod xxxx True False\n",
      "technique technique NOUN NN dobj xxxx True False\n",
      "( ( PUNCT -LRB- punct ( False False\n",
      "2020 2020 NUM CD appos dddd False False\n",
      ") ) PUNCT -RRB- punct ) False False\n",
      "Computers Computers PROPN NNPS appos Xxxxx True False\n",
      "& & CCONJ CC cc & False False\n",
      "Chemical Chemical PROPN NNP compound Xxxxx True False\n",
      "Engineering Engineering PROPN NNP conj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "134 134 NUM CD appos ddd False False\n",
      "; ; PUNCT : punct ; False False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Abdeljaber, O., Avci, O., Kiranyaz, M.S., Boashash, B., Sodano, H., Inman, D.J., 1-D CNNs for structural damage detection: Verification on a structure health monitoring benchmark data (2018) Neurocomputing, 275, pp. 1308-1317; Arunthavanathan, R., Khan, F., Ahmed, S., Imtiaz, S., Rusli, R., Fault detection and diagnosis in process system using artificial intelligence-based cognitive technique (2020) Computers & Chemical Engineering, 134;\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.span.Span' object has no attribute 'check_flag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m doc \u001b[39m=\u001b[39m nlp(\u001b[39m\"\u001b[39m\u001b[39m1-D CNNs for structural damage detection: Verification on a structure health monitoring benchmark data (2018) Neurocomputing, 275, pp. 1308-1317; Arunthavanathan, R., Khan, F., Ahmed, S., Imtiaz, S., Rusli, R., Fault detection and diagnosis in process system using artificial intelligence-based cognitive technique\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m doc[:]\u001b[39m.\u001b[39;49mcheck_flag(IS_TITLE)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.span.Span' object has no attribute 'check_flag'"
     ]
    }
   ],
   "source": [
    "from spacy.attrs import IS_TITLE\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"1-D CNNs for structural damage detection: Verification on a structure health monitoring benchmark data (2018) Neurocomputing, 275, pp. 1308-1317; Arunthavanathan, R., Khan, F., Ahmed, S., Imtiaz, S., Rusli, R., Fault detection and diagnosis in process system using artificial intelligence-based cognitive technique\")\n",
    "doc[:].check_flag(IS_TITLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import certifi\n",
    "import spacy\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "\n",
    "if not os.environ.get(\"PYTHONHTTPSVERIFY\", \"\") and getattr(ssl, \"_create_unverified_context\", None):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy' has no attribute 'download'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spacy\u001b[39m.\u001b[39;49mdownload(\u001b[39m'\u001b[39m\u001b[39men_core_web_trf\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'spacy' has no attribute 'download'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.download('en_core_web_trf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
