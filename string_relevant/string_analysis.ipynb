{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "systems,manyeffortshavebeenmadetopreventaccidents,escalations,anddominoeffects.Correctfaultdetectionandpreciselydiagnosisgivevaluableinformationtotheoperatorsandsaveasignificantamountoftimewhenanalarmisnoticed,whichisoneoftheprimaryoperationaltaskstoenhancesafetyandreliability(Aminetal.,2021a).Faultdetectionanddiagnosis(FDD),therefore,thecoreelementsofprocessmonitoringtoavoidingabnormalsituationsandaccidents,hasbeenresearchedextensively(Ge,2017;Abidetal.,2021;Seversonetal.,2016;Kongetal.,2022;SongandJiang,2022).Generally,FDDmethodscanbeclassifiedasmodel-based,knowledge-based,anddata-drivenmethods(Alauddinetal.,2018;Yinetal.,2019;KongandGe,2021;Aminetal.,2021b).Inaddition,FDDalonedoesnotdirectlymeasuresafetynorprovideanyindicationofassociatedriskofthefault,whichishighlydesirablefromsafetyanalysisperspective(Aminetal.,2020a;Arunthavanathanetal.,2021).Recently,variousrisk-basedhybridFDDmethodologyhavebeenproposed(Arunthavanathanetal.,2021)basedonPCA,self-organizingmap(SOM)(Yuetal.,2015),naïveBayesclassifier(NBC)andeventtree(Aminetal.,2020b),thevinecopula-basedmethods(Aminetal.,2021b;QiaoweiandYanting,2019;ZhouandLi,2018;Renetal.,2015).However,theever-expanding,highlycoupledandinterdependentsubsystemsandcomponentsofindustrialprocessesmakethesystemsmuchmorecomplex(Sunetal.,2022;YangandGe,2022)andpossiblydysfunctionalcomponentsfaultmayexhibitmorefrequently(YangandGe,2022;Mamuduetal.,2021;Khanetal.,2020).TheconstantlypromotingdemandsofbetterperformanceofFDDrequiresmorepowerfulmethods(SongandJiang,2022;Lietal.,2020;Jiaetal.,2018).SincetheachievementofAlexNetonImageNetLSVRC-2012,LongShort-TermMemory(LSTM)neuralnetwork(Zhaoetal.,2018),GatedRecurrentUnit(GRU)(YuanandTian,2019)andConvolutionalneuralnetwork(CNN)havebeenappliedinFDDofchemicalprocesssince2018(WuandZhao,2018).Thesedeeplearning(DL)methodsregardFDDasaclassificationproblemandstrivetoimprovethecorrespondingaccuracybyfinedesignedmodelstructure.DLreducestherequirementforpriorknowledgeinFDDandopensupbroadprospectsinthisfield.Lateron,hybridstructureslikebidirectionalrecurrentneuralnetwork(BiRNN)withGRUandLSTMmodel(Zhangetal.,2019),CNN-LSTM-fusedmodel(Wangetal.,2020;Huangetal.,2022),and*Correspondingauthorat:SchoolofChemicalEngineeringandTechnology,TianjinUniversity,Tianjin300072,China.E-mailaddress:ksong@tju.edu.cn(K.Song).1TheseauthorscontributedequallytothisworkContentslistsavailableatScienceDirectProcessSafetyandEnvironmentalProtectionjournalhomepage:www.journals.elsevier.com/process-safety-and-environmental-protectionhttps://doi.org/10.1016/j.psep.2022.12.055Received12November2022;Receivedinrevisedform3December2022;Accepted19December202ProcessSafetyandEnvironmentalProtection170(2023)660–669661wavelet-CNN(WCNN)(Lietal.,2020)havebeenproposedtoimprovethediagnosticperformancebyfusionadvantagesfromdifferentbackbonenetworkstructures.Somevariantstructuresofautoencoders(AE),suchasstackedsparseautoencoder(SSAE)(Zhangetal.,2018),ladderdiagramautoencoder(LAE)(ZhangandQiu,2022),convolutionalgatedrecurrentunitautoencoder(CGRU-AE)(YuandLiu,2020),adversarialautoencoder(AAE)(Jangetal.,2021),etc.,havebeenappliedtoFDDduetotheirpowerfulfeaturelearningandrepresentationcapabilities.Inaddition,DLhasalsoshownstrongcapabilitiesinmanymoresubdividedtasksofFDD.Forexample,usegenerativemodelstosolvetheproblemoflimitednumberofsamplesforsomeunbalanceddatasets(Pengetal.,2020),usetransferlearningtorealizemulti-modeFDD(WuandZhao,2020),interpolateincompletedata(Guoetal.,2020),andconductinterpretablemodelstoexploretherelationshipbetweenfaultsandvariables(Lietal.,2021;Agarwaletal.,2021),etc.AmongtheseDLmethods,theconvolutionalstructurehasbeenthemostimportantone.VariousmodelshaveincorporatedconvolutionalstructuresorusedCNNasbackbonenetworks.TheuniqueweightsharingandsparsityofconnectionsofCNNgreatlyreducethenumberofparametersneedtobeoptimizedandimprovethemodelgeneralizationability(Schmidhuber,2015).Althoughdenseblock(Huangetal.,2017),depthwiseseparableconvolution(Howardetal.,2017)andotherdozensadvancedversionsofCNN(Liuetal.,2022)havebeendevelopedtoimprovetheirperformance,butthegeneralideaofaCNNnetworkistoexpandthelocalreceptivefieldandcombinehigh-levelfeaturesbystackingconvolutionallayersanddownsampling.Limitedbylocalreceptivefield,therefore,itisimpossibletoachieveanultra-deepconvolutionstructure(morethantwentylayers)tocaptureslowdrift,longperiodicandothertime-varyingfeaturesnortocaptureunknowncomplicatedrelationshipsamongprocessvariables.Onthecontrary,Transformer,originallyproposedbyVaswanietal(Vaswanietal.,2017).,hasbeenconsideredamilestoneinthedeeplearningfieldbecauseitreliesonlyontheattentionmechanismratherthantherecurrentandconvolutionfunctions(Kimetal.,2017).Lateron,byextendingtokenEmbeddingtopatchEmbeddingandcombiningtheclasstokenmethodoftheBidirectionalEncoderRepresentationsforTransformers(BERT)model,VisionTransformer(ViT)successfullyextendedTransformertothefieldofCV(computervision)(Dosovitskiyetal.,2020).Theglobalreceptivefieldandself-attentionmechanismsuccessfullymakeuptheshortcomingsofCNNbasedmethods.ComparisonsontheperformanceofTransformerbasedandCNNbasedmethodshavebeenmadeindifferentapplications(Raghuetal.,2021).Notunexpectedly,DLframeworksintegratingbothconvolutionandattentionmechanismshavealsobeenproposedandappliedindifferentapplications(Zhengetal.,2021;Pengetal.,2021)anditgraduallybecomeatrendforsuchintegratingwhereveritneedstoimprovetheperformanceofDLmethods.FDDforindustrialprocesseswithvariablevaluessampledbyDCS(DistributedControlSystem)ismorelikeatime-seriesissue.Correspondingly,Transformermodel,whichwasoriginallydesignedfor“sequentialanalysis”(Vaswanietal.,2017),issupposedtoachieveabetterperformancethanCNNbasedmethods.AlthoughtheheavyrequirementontrainingsamplesizesandparametersmaymakeTransformer-basedmethodslesspractical,buttheglobalattentionabilityprovidedbytheglobalreceptivefieldandtrainableself-attentionmechanismofTransformerbasedmodelaresupposedtoextractrealimportantdeepfeaturesrepresentingcomplicatedrelationshipsamongvariablescausedbytightlycoupledcomponentsandslowdriftorperiodicfaultscausedbycorrosion,fatigue,andsoonincompliedchemicalengineeringprocesses.TorevealthepotentialoftheglobalattentionmechanismofTransformerbasedmethod,anewIPO-ViT(IndustrialProcessOptimizationViT)methodwasproposedbasedonViTmethodandwasappliedinFDDofTennesseeEastman(TE)processandofaR-22process,arefrigerantproducingprocessofafluorochemicalengineeringfactorylocatedinEastChina.ThensixdifferentvariantsintegratingbothconvolutionandattentionmechanismshavebeenproposedandappliedinFDDofTEprocess.Wedemonstratedthatawell-designedtransformerstructurecanachievethebestFDDperformancewithoutincreasingsampleandcomputationrequirements.Notonlythat,intheexperimentofthevariantmodelsthatfuselocalreceptivefieldandglobalattention,theinfluencingfactorsthataffecttheperformanceofthemodelaretheoreticallyanalyzed.Fornetworkarchitectureofsuchcomplexityasdeeplearning,simplycombingconvolutionandself-attentionfunctionsdoesn’tguaranteebetterperformance.Ouranalysisshowedthatthelocalattentionexplosion,informationalignmentandexpressivecapabilityarethreemajorchallengesforsuchcombinations.TheseinsightsarevaluablenotonlyforFDDbutalsofordata-drivenriskassessmentmodelstryingtousedeeplearningmethods.Thepaperisorganizedasfollows.Section2describesdetailedinformationonViT-basedmethod,sixdifferentvariantsintegratingbothlocalreceptivefieldsandglobalattentionmechanisms.ThenthecorrespondingresultsandcomparisonsonthefaultdiagnosisaccuracyofR22andTEbenchmarkobtainedbytheproposedbasicmethodsandotherlatestFDDmethodsarelistedinSection3,alongwithadiscussionofvariantmodels.Atlast,theconclusionsaresummarizedinSection4.2.Methods2.1.VisiontransformerTheregularstructureofabasicViTisshowninFig.1(a).ViTcanbedividedintothreeparts:Thefirstpartistheembeddingblock,whichmatchestheinputoftheTransformerbyconvertingtherawdataintomultipleone-dimensionalvectors;Thesecondpartisthefeatureextractionblock,whichbuildsglobalattentiontoinputdatabystackingmultipleTransformerblocks;Thelastpartistheclassificationblock,whichcompletesthedecisionboundarydivisioninthefeaturespacethroughMulti-layerPerceptron(MLP),soastorealizeFDD.Thedetailedprocessisdescribedasfollows.2.1.1.EmbeddingblockAsquareresolutionpatchisusedtoreshapea2Dsamplematrixx∈RH×W×Cintoasequenceofflattenedpatchesxp∈RN×(P2×C),where(H,W)istheresolutionoftheoriginalsamplematrix,Cisthenumberofchannels,(P,P)istheresolutionofeachpatch,andN=HW/P2isthenumberofpatches.Atrainableclassificationtoken(CLS)isconcatenatedbeforethesequenceofembeddedpatches,whosestateattheoutputlayeroftheTransformerencoderservesastheFDDresults.Byusingasemantic-freeclassificationtoken,thesubjectivenesscausedbyassigninganoutputembeddingfromthelastTransformerblockcanbeovercome.Thenatrainable1Dpositionembeddingisintroducedforeachembeddedpatchtoretainpositioninformation.2.1.2.FeatureextractionblockThisblockconsistsofmultipleTransformerblocksstackedtogether,anditsarchitectureisshowninFig.1(b).EachblockcontainsMultiheadself-attention(MSA)andMLPwithaGELUnon-linearity,whileLayerNormalization(LN),Dropout,andResidualconnectionsareappliedforeveryblock.Amongthem,Transformer’sglobalattentionbenefitsfromMSA.Theembeddingvectorofeachpatchachievesattentionbycomputinga\"Query-key\"pairwitheachoftheremainingvectors.TheMulti-headself-attentionmechanismcanbedescribedbythefollowingEqs.(1–3)(Vaswanietal.,2017;Linetal.,2017):Attention(Q,K,V)=softmax(QKT̅̅̅̅̅dk√)V(1)K.ZhouetalProcessSafetyandEnvironmentalProtection170(2023)660–669662Hi=Attention(XWiQ,XWiK,XWiV)(2)MSA(Q,K,V)=Concat(H1,H2,...,Hn)Wo(3)whereQ,K,andVrepresenttheQuery,Key,andValuematrix,respectively.MultiplyingXbytheweightmatrixWiQproducesQi,the“Query”matrixassociatedwiththefeaturematrixX,thenKiandVimatrixproducedbylineartransformationmatricesWiKandWiV,respectively.Thedot-product(multiplicative)attentionmechanismisidenticaltoTransformermodelinviewofitsspeedandspace-efficiency.The“focusscore”calculatedbyQandKisdividedbythesquarerootofthedimensionofthekeyvectorsK,whichleadstohavingmorestablegradients.TheSoftmaxinEq.(1)determineshowmucheachfeaturewillbeexpressedatthisposition,anditwillbemultipliedbythevaluematrixVforpreparationtoweightedsumallfeaturesup,whichistheattentionresultsofHioftheheadi.Finally,Woisthelineartransformationmatrixformergingtheresultofthetotalnheadsintoone.Thecorrespondingcalculationforlayerlisdefinedasz′l=MSA(LN(zl1))+zl1(4)zl=MLPLNz′l))+z′l(5)2.1.3.ClassificationblockTheclassificationtokenhasbeenextractedafterthelastTransformerblock,thenoneMLPlayerissetforbetterrepresentationperformance.Thisblockfollowsdefaultmulti-classclassificationsetting.2.2.IPO-ViTModelandsixdifferentvariantsofViT2.2.1.IndustrialprocessoptimizationViTbasedmodel(IPO-ViT)Limitedbythedatavolumeofindustrialdata,andconsideringthecomputationalrequirementsinpractice,anIPO-ViTmodelstructureisdesignedaswhatisshowninFig.1a.TobetteradaptViTtoindustrialprocessdata,thefollowingimprovementsareappliedtothemodel:Itisworthnotingthat,unliketheimage,therowsandcolumnsofanindustrialprocessinformationmatrixarenotequivalent.Firstly,therefore,toextractinformationamongvariablesineachtimestep,arectangularpatchoperationisadoptedinIPO-ViT.Eachpatchcontains1×ndatafromtherawmatrix(i.e.,processvariablesvectorinonetimestep),andmpatchesaregeneratedbythisoperation.ThenthesepatchesaremappedtoD-dimensionvectorswithatrainablelinearprojection,respectively.Thisrealizesthedecouplingofvariablesandsamplingfromtheperspectiveofembedding,thusensuringthattheinformationisalwaysalignedduringtheinformationflowintheentiremodel.Aftercomprehensiveexperimentalevaluation,theoptimalmodelisobtainedunderthebalanceofaccuracyandcomputationamount.SpecificdetailsarelistedinAppendixS1.2.2.2.VariantsofIPO-ViTForregularViT,theattentiondistanceofallheadsinViTincreaseswhenthenetworkdepthincreases.ThismayleadtoanimprovedabilityofanViTmodeltoextractglobalfeaturescontainedintheoriginaldatabutmayalsoincreasetheriskofignoringlocalbutimportantinformationinherentinthesedata.Onthecontrary,CNNisfamousforFig.1.(a)TheoverviewoftheproposedIPO-ViTmodel;(b)ThearchitectureoftheTransformerblock.K.ZhouetalProcessSafetyandEnvironmentalProtection170(2023)660–669663extractinglocalfeaturesbythelocalreceptivefieldofitsconvolutionfunctions.TounraveltheprinciplesincombiningconvolutionfunctionsandTransformerblocksforperformanceimprovementinFDD,severalnetworkstructuresweredesignedtointegrateCNNintoIPO-ViTmodel.TheirbriefnetworksareshowninFig.2.1.Variant(a):InsertaCNNbranchpathbeforethefirstlayerofthebasicIPO-VITmodel.ItmayextractthelocalinformationfromtherawdatabyconvolutionallayersbeforeitisinputintothefollowingIPO-VITmodel.Atthesametime,becausethefeaturesextractedbytheshallowCNNaremorelocalizedandeasiertounderstandandusebythetransformer,andconsideringthelimitationofthecomputationandparameteramountinindustrialpractice,theCNNbranchpathshouldnotbetoodeep.Therefore,threeconsecutiveconvolutionallayersareusedtoprovidelocalinformationfortheoverallmodel.Inaway,theCNNbranchpathwedesignedresemblesa’stem’layer,whichisanessentialcomponentinnetworkdesign.2.Variant(b):Variant(a)mayextractthelocalinformationfromtherawdatabyconvolutionallayersinsertedbeforetheIPO-VITmodel.Butitmaylosetherawglobalinformationbecauseoftheinsertedconvolutionallayerstoo.DifferentfromVariant(a),therefore,askipconnectionisintroducedfromtherawdatatothefeaturemapsbeforebeinginputintotheIPO-VITmodel.Theconnectionbetweenchannelsisusedtoretaintheoriginalinformation,andfinallyitisinputintothetransformerblockthroughembedding.Inthisway,bothglobalandlocalfeaturesmayextractbythefirstCNNandthefollowingTransformerblocks.NotethattheCNNstructureisthesameasinVariant(a).3.Variant(c):ACNNbranchpathisjuxtaposedwiththeTransformerblocktoextracttheglobalandlocalfeaturesparallelly.Thenthesefeaturesarefusedtogetherrightbeforetheyareinputintothelastlayerofclassificationblock(calledtheclassificationlayer).Inthisway,theinformationdealedbytheTransformerbranchandtheCNNbranchdonotinteracttoeachother,sothatthetaskofinformationfusionandeffectiveutilizationisconcentratedintheclassificationlayer.NotethatalightweightMobilenetv2(Sandleretal.,2018)wasusedinthebranchpathconsideringthereasonablecomputationandparameteramount.4.Variant(d):SimilarwithVariant(c),butaskipconnectionusedbetweeneachTransformerblockandCNNblock.Inthisvariant,thelocalfeaturesextractedbyCNNareinputintothetransformerblockhierarchicallyafterembedding.Itissupposedtointegratelocalfeatureswithglobalfeaturesprogressivelyinsteadofmergingallfeaturesbeforethelastlayer,whichmayassistthetransformertoperformmoreeffectivefaultdiagnosis.5.Variant(e):InspiredbytheWCNN(Lietal.,2020),awavelettransformisintroducedbeforethebaseTransformermodel.Itmayfilterouthigh-frequencynoiseinthetimeseriesandhelpovercomemodeloverfitting.Inaddition,thewavelettransformisusuallyhelpfulforthediagnosisofsomefaultswhosefeaturesarelikethoseofthenormalstate.6.Variant(f):SimilarwithVariant(e),butaskipconnectionisintroduced.RawdataanddataprocessedbywaveletwereintegratedbeforebeinginputintothebasicIPO-VITmodel.Insummary,ourgeneralideaistounravelguidelinesforcombiningthelocalfeatureextractionabilityofCNNandtheglobalattentionofViT.Inordertobetterexplorethewayofinformationfusion,theskipconnectionsinvariant(b-d)andvariant(f)areusedtoreservetheoriginalinformationasbestaspossible.Inaddition,itisworthnotingthat,incontrasttopreviousstudy,variants(e-f)weredesignedtokeepthe“one-stage”FDDinourstudy,whichcontainsaserialofwavelettransformwith/withoutconcatenationoperations.2.3.EvaluationcriteriaAsaspecificmulti-classclassificationtask,theperformanceofeachmethodisevaluatedbyfaultdiagnosisrate(FDR)andfalsepositiverate(FPR),whicharedefinedasfollows(WuandZhao,2018;ZhengandZhao,2020):FDR=TPTP+FP(6)FPR=FNFN+TN(7)Forthei-thcategory,FDRrepresentstheratioofthenumberoffaultsamplescorrectlypredictedasthei-thcategory(TruePositive)tothetotalnumberofsamplesbelongingtothatcategory(TruePositiveandFalsePositive).Itisalsocalledtruepositiverate(TPR);FPRistheratioofthenumberoffaultsampleswronglypredictedasothercategories(FalseNegative)tothetotalnumberofthesamplesbelongingtoothercategories(FalseNegativeandTrueNegative).3.Resultsanddiscussion3.1.FDDperformanceofglobalattention-basedmethodsvslocalreceptivefieldmethodsTotesttheperformanceofglobalattention-basedmethods,inthissection,threestate-of-the-artdeep-learning-basedFDDmethodswereFig.2.Thevariantnetworkstructures.Eacharrowpointingtoalinemeansaconcatenationoperation.Linearprojection,classtoken,andpositionembeddinghavebeenomitted.K.ZhouetalProcessSafetyandEnvironmentalProtection170(2023)660–669664usedascomparationswiththedatafromaR-22(HCFC-22)producingprocess,arealcomplexchemicalindustrialprocess,andfromtheTEprocess,awidelyusedbenchmark.TheyaretheDCNNmethod(WuandZhao,2018),theCNN-LSTMmethod(Huangetal.,2022)andtheWCNN(Lietal.,2020).DCNNmethod(WuandZhao,2018)isaCNN-basedone-stagemethodanditisthefirstinvestigationofCNNinFDD.TheCNN-LSTMmethod(Huangetal.,2022)isarepresentativeofmulti-technology-fusedstructurebutone-stagemethods.WCNN(Lietal.,2020)isaCNN-basedbuttwo-stagemethod.ThestructuresoftheseFDDmethodsarelistedinAppendixTableS1.Allmodelsweretrainedandtestedusingthesameworkstation(Linux,Inteli9–9980XE,NVIDIATITANRTX,and128GBRAM).3.1.1.R-22fluorochemicalengineeringprocessR-22(HCFC-22)hasbeenacommonrefrigerantandpropellantfordecadesdespitethecontrollingandpossiblyeradicatingofthisusagetoavoidtheadverseimpactonozonedepletionandrelatedclimateandenvironment.R-22productionhasshownstablegrowthbecauseithasalsobeenanirreplaceableversatileintermediateinindustrialorganofluorinechemistry(Lietal.,2020;KudomaandTekere,2021).R-22formedbythechemicalreactionasEq.(8),thenispurifiedbywaterandalkalitoremoveresidualHClandHF.ThemainoperatingunitsofaR-22producingprocessincludeafeed,areactor,tworectifyingcolumns,awaterscrubber,andaseparator.TheproductionprocessofitisshowninFig.3.CHCl3+2HF̅̅̅̅→SbCl5CHClF2+2HCl(8)AsachemicalprocesswithcorrosiveandtoxicmaterialsandbyproductslikeHFandHCl,itiscriticalforbothpublicsafetyandenvironmentalprotectiontoensureproducingprocesssafety.AdvancedprocessmonitoringsystemhasbeenappliedforsecuringthesafetyoftheR-22producingprocess.WhatismoreimportantisthatbecauseHFandHCLarebothstrongacidwithstrongcorrosiveness,thelifeofacertainimportantpartofR22producingprocessisusuallylessthantenmonths.Itmeansthereisaveryslowtime-varyingdriftinherentinsamplevalues.Additionally,complicatedrelationshipsexistingamongvariables.Tounravelwhetherglobalattention-basedmethodshavesuperioritiesoverCNN(localreceptivefieldmethods)onFDDwithsuchslowtime-varyingdriftdata,theabovementionedthreestate-of-artlocalreceptivefieldmethods(DCNN(WuandZhao,2018),CNN-LSTM(Huangetal.,2022)andWCNN(Lietal.,2020))andtheproposedIPO-ViTmodelwerecompared.Duetotheconfidentialityagreement,only10mostrelevantvariablesofthereactorR-301,whichhasthebiggestimpactontheentireproductionprocess,wereusedinthisstudy.AlldataweresampledfromafluorinationplantlocatedinEastChinabetweenMay2019andNovember2019.ThevaluesofthesevariablessampledbyDCS(distributedcontrolsystem)everyminuteduringnormaloperationconditionandfivedifferenttypesofabnormalconditionswereusedtotrainandtestFDDmodels.Notethatthesefiveabnormalcaseswerenotcriticalfaultsandnotseriousenoughtocausedamages.Foreachcondition,5000samples(i.e.,100datamatrices)wereincluded.80%ofthemwereusedtotrainFDDmodelswhiletheother20%wereusedtotestthem,andthesplitofthedatasetisrandom.Inaddition,bothtrainandtestdatasetswerestandardizedbythemeanandvarianceofthetrainingdataset,respectively.Inthetrainingstage,allthesedeeplearningmodelslistedaboveareoptimizedbySGDwithmomentumandNesterovwhoselearningrate,weightdecay,andmomentumweresetas8E-5,1E-6,and0.9,respectively.Categoricalcross-entropywasusedasthelossfunction,whichcouldmeasuremilddifferencesandprovidethedirectiontooptimize.Thebatchsizewassetas256.PleaserefertoAppendixS1formoredetails.ThecomparisonresultswerelistedinTable1.Abnormalcases4and5arecomparativelyeasytodiagnosis.Allmethodscandiagnosisthemwith100%FDR.Butforabnormalcases1,2,and3,localreceptivefiledbasedmethodsdidn’tperformverywell.ThelowestFDRobtainedbythemwereonly85%,90%and80%.Especially,theCNN-LSTMmethodobtainedthelowestFDRamongallmethods.Evenafterbeingfine-tunedinl2regularization,learningrate,andsoon,itstillcouldn’tobtainahigherFDR.GlobalattentionmodelshavelessinductivebiasthanCNNs.ThepoorperformanceofCNNsmaybeduetotheirinherentinductivebiases,suchastranslationinvariance,Fig.3.TheproducingprocessofR-22.K.ZhouetalProcessSafetyandEnvironmentalProtection170(2023)660–669665whichdonotfitwellinindustrialprocessdata.Itmayalsobecausedbythesmallsampleamountofdatasetortheinsufficientexpressivityofthemodel.Incontrast,theglobalattention-basedmethod(IPO-ViT)gotthehighestFDRandlowestFPR(0.9917and0.0017),whichattributedtothecapacityandstudyabilityintime-varyingdataofournetworkstructure.Specifically,itreportedaperformancewith0.9833FDRand0FPRonaverageinthesethreedifficultabnormalcases,whichweremuchbetterthan0.9500FDRand0.0036FPRonaverageprovidedbyWCNN.TheconfusionmatrixandvisualanalysiscanbefoundinAppendixS3.Additionally,globalattention-basedmodelsaregenerallyconsideredtoperformpoorlyonsmalldatasets.Butinourstudy,only5000*80%=4000samplesforeachabnormalconditionswereusedtotrainthemodel.ThisamountofdatawasverysmallcomparedtotheamountofdatausedbyotherapplicationsofconventionalTransformermodels(Huangetal.,2022;Wortsmanetal.,2022).Therefore,theseresultsshowthatourproposedIPO-ViTmodelcanachievebetterFDDperformancewithrelativelyfewsamples.TheinferencetimegranularityoftheproposedIPO-ViTmodelcanbedecreasedto1min(sameasDCSsamplinginterval).Moreover,usingtheOnlineQueueAssemblyUpdatingMethod,originallyproposedbyus(Lietal.,2020),theIPO-ViTmethodcandiagnosetheabnormalconditionafter16minandstablytriggeranalarmafter21min(usingabnormalcase2asexample).Ofcourse,thetimedelaywasstilltoolongcomparedwithPCA(principalcomponentanalysis)andotherlinearmethods,butitwasindeedahighlyreducedonecomparedwiththeexistingdeeplearningmethods(normallywithonehourdelay).Atleast,globalattention-basedmethodsarenotlesspracticalthanlocalreceptivefieldmethodsconsideringboththetrainingsamplerequirementandtheinferringtime.3.1.2.TennesseeEastmanProcessTheTennesseeEastman(TE)Processisaplant-wideindustrialbenchmarkdesignedbytheEastmanChemicalCompany,whichcanprovidemassiveamountsofsimulatedprocessdataforadvancedprocesscontrolstudies.IthasbeenextensivelyusedtoverifytheperformanceofvariousFDDmethods(DownsandVogel,1993).Fig.4illustratesthediagramoftheTEprocess.Itcontainsfivemajorunits:areactor,astripper,acondenser,arecyclecompressor,andaseparator.TheTEprocessprovided12processmanipulatedvariables,22continuousprocessmeasurements,and19componentanalysismeasurements.Intermsoffaulttypes,thereare28processfaulttypesavailableintherevisedversion(https://depts.washington.edu/control/LARRY/TE/download.html)(Batheltetal.,2015),butonlyIDV1-IDV20(listedinAppendixTableS2)wereanalyzedinthisstudyforafaircomparisonwithotherpublishedmethods.Bothnormalandfaultsimulationdataweregeneratedbythemode3simulatoroftheTEprocessinMATLAB2016a.Thesamplingintervalwassetto50samples/hourandtheperiodofeachdatamatrixwassetto1h(i.e.,50samplesperdatamatrix).Becausethevaluesofthetwoprocessmanipulationvariables(XMV(9)andXMV(12))inthisTEmodearefixedthroughoutthesimulationprocess,theywereremovedfromthedataset.Therefore,only51variablesweremonitored,andthedimensionofeachdatamatrixwas50×51.Undernormalconditions,thesimulatorrunsfor50htogeneratedata.ForeachIDV,thesimulatoralsoranfor50h,butafaultwasintroducedafter10hofsimulation.Onlydataofthelatter40hwithTable1FaultdiagnosisperformanceforR-22processobtainedbyfourdifferentmethods.ConditionDCNNFDR/FPR(%)CNN-LSTMFDR/FPR(%)WCNNFDR/FPR(%)OursFDR/FPR(%)Normal95.00/0.00100.00/2.00100.00/0.00100.00/1.00Abnormal190.00/1.0085.00/1.0095.00/0.0095.00/0.00Abnormal290.00/0.00100.00/3.00100.00/0.71100.00/0.00Abnormal3100.00/4.0080.00/1.0090.00/0.36100.00/0.00Abnormal4100.00/0.00100.00/0.00100.00/0.00100.00/0.00Abnormal5100.00/0.00100.00/0.00100.00/0.00100.00/0.00Average95.83/0.8394.17/1.1797.50/0.1899.17/0.17Fig.4.P&IDoftherevisedTEprocess.K.ZhouetalProcessSafetyandEnvironmentalProtection170(2023)660–669666faultarecollected.Inaddition,IDV6(rootcauseistheexcessivereactorpressure)canmaketheTEprocessshutdownwithin6h.Thus,processdataofIDV6arecollectedforonly6h.Bothnormalandfaultconditionrun10timesindependentlywithdifferentsetpointstocomprehensivelycoverdifferentworkstate.Therefore,50h×10timesdatamatrixfornormalcondition,and40h×10timesforeachfaultcondition(6h×10timesforIDV6)weregenerated.ThetotalnumberofIDVsampleswas383,000andofnormalsampleswas25,000.Inaddition,datasetsegmentationandstandardizationrulearethesameastheR-22processinSection3.1.1,thenthetestdatasetwasappliedtoevaluatetheperformance.ThemonitoringresultsofthetestdatasetobtainedbyDCNN(WuandZhao,2018),CNN-LSTM(Huangetal.,2022),WCNN(Lietal.,2020),andtheproposedIPO-ViTmethodarelistedinTable2.ForallIDVs,ourproposedIPO-ViTmethodobtainedthehighestdiagnosticaccuracysofar,increasingtheFDRfrom92.5%to94%.ItisobviousthattheproposedIPO-ViTmethodhasachievedthehighestdiagnosticaccuracyamong16IDVsandhasthehighestnormalstatedetectionaccuracy.IDVs3,9,15,16havebeenconsidereddifficulttodiagnoseinpreviousstudies(WuandZhao,2018;ZhangandZhao,2017;Yinetal.,2012).a)ForIDV3,exceptforDCNN,othermodelshaveobtainedFDRhigherthan90%.TheproposedIPO-ViTmodelstillachievesthehighestdiagnosticaccuracy(3.75%FDRup),reducingtheerrorrateby42.8%.b)ForIDV9,thediagnosisperformanceforneitherDCNNnorCNNLSTMaregoodenough.ButtheproposedIPO-ViTmodelobtainsanFDRhigherthan70%andeffectivelyreducedFPRmorethan50%comparedtoothermodels.c)ForIDV15,DCNNonlyobtained30%FDR,buttheproposedIPO-ViTmodelimprovesthediagnosisaccuracytohigherthan70%.d)ForIDV16,thediagnosticaccuracyoftheproposedIPO-ViTmodelisslightlylowerthanthatofCNN-LSTMandWCNN,butitstillobtainsthelowestFPR.Theaboveresultsprovedthatthemodelbasedonglobalattentionisbetterthanthelocalreceptivefieldmodelinlong-distancemodeling,andcanmoreeffectivelyextractthedeepcouplingrelationshipamongvariables,thusobtainingstrongerfaultdetectioncapability.Inaddition,onlinefaultdetectionperformancewasalsoevaluatedintheTEprocess.IDV6waschosenasanexample,whichcanmaketheprocessautomaticallyshutdowninonly6hafteritwasintroduced.ByusingtheOnlineQueueAssemblyUpdatingMethod(Lietal.,2020),analarmwasfirsttriggeredat20.4minsandwasstablyalarmedafter24mins,whichshowelevatedreal-timeperformancethanothertraditionalonlineFDDmethods(morethan60mins).TheconfusionmatrixandvisualanalysiscanbefoundinAppendixS3.Inaddition,theamountofdatausedinthisexperimentisn’tmuchbiggerthanthoseforCNNkindsofmethods.Theresultofmorethan1.5%FDRimprovementfurtherprovesthatthewell-designedglobalattention-basedmodelcanoutperformthelocalreceptivefield-basedmodelsatthesamelevelofdatavolume.3.2.PerformanceofthevariantintegrationofglobalattentionandlocalfeatureextractingmethodsonFDDNowthatconvolutionisfamousforitslocalreceptivefieldwhilemulti-headattentionisfamousforitsglobalattention,naturally,thecombinationsofthemaresupposedtocomplementeachother.Sixdifferentvariantnetworkstructuresweredesignedtounravelguidelinesfortheintegrationofthem.SinceIPO-ViTmodelcandetectanddiagnosetheabnormaleventsinR-22producingprocessbyover94%faultdiagnosisrate.Thereisn’ttoomuchroomforthesevariantstoshowtheirimprovementonFDDperformanceforR-22data.Additionally,TEdatacanbeavailableforfurtherstudiesandcomparisonsforanyotherresearchers.Therefore,thefollowingcomparisonswereperformedusingTEprocessdata.TheFDDresultsareshowninTable3.InVariant(a),rawdataisfirstlyinputintolayersofconvolutionfunctions,thenextractedfeaturesareinputtothefollowingTransformerblocks.ThelocalreceptivefieldinherentinCNNlayersinsertedbeforetheTransformermodulesissupposedtoextractlocalfeatureswhichwillbehighlyprobablyignoredbyTransformeritself.ButaccordingtotheresultsshowninTable3,bothFDRandFPRobtainedbyVariant(a)aremuchlowerthanthoseobtainedbyIPO-ViT.ItmeansthelimitedreceptivefieldbroughtbytheCNNlayersinsertedbeforetheTransformermodulescounteractedtheglobalreceptivefieldofthem,makingitdifficulttoreflecttheadvantagesoftheTransformer.Theoretically,itmaybecausethelocalinformationfusionbroughtbyconvolutionmayleadtoincorrectself-attentioncomputationanddestroythealignmentofvariableandsampleinformationintroducedbyourproposedIPO-ViT,whichmaybecausedbythefollowingtworeasons.3.2.1.ThelocalattentionexplosionSpecifically,sincetheessenceoftheconvolutionkernelisalocalmatrixoperationwithaslidingwindow,theinformationofeachpointintheoutputfeaturemapiscomposedofinformationfusionofeachpointwithintherangeofthelocalreceptivefieldwithitasthecenterpoint.Inotherwords,theinformationofeachpointoftheoriginalinputisdistributedinasquareareaofthelocalreceptivefieldrangecenteredonit.Therefore,foraglobalattentionmethodsuchasTransformer,whencalculatingtheattentionofeachsample,thesampleswithintherangeofthelocalreceptivefieldcenteredonitwillhaveahigherattentioncoefficient.Atthesametime,asshowninEq.(1),thecalculationoftheattentioncoefficientrequirestheusageoftheSoftMaxfunction,sothelargerlocalattentionwillweakentheattentionofsamplesthatarefarapartandmayhaveagreatercorrelationtofaultdiagnosis,thusdestroysTable2FaultdiagnosisperformanceforTEprocessobtainedbyfourdifferentmethods.ConditionDCNNFDR/FPR(%)CNN-LSTMFDR/FPR(%)WCNNFDR/FPR(%)OursFDR/FPR(%)Normal90.00/1.5091.00/0.7891.00/1.7092.00/0.65Fault0298.75/0.0697.50/0.0697.50/0.0097.50/0.00Fault0357.50/3.0391.25/1.0391.25/0.9095.00/0.90Fault0895.00/0.2698.75/0.3993.75/0.1398.75/0.00Fault0938.75/2.2655.00/1.2968.75/1.2971.25/0.64Fault1081.25/0.1986.25/0.1981.25/0.0093.75/0.00Fault1198.75/0.00100.00/0.0098.75/0.0098.75/0.00Fault1297.50/0.0697.50/0.0698.75/0.13100.00/0.00Fault1388.75/0.1987.50/0.0691.25/0.6495.00/0.39Fault1530.00/3.2968.75/1.9368.75/2.7171.25/1.93Fault1668.75/2.0080.00/1.1676.25/1.0375.00/1.03Fault1795.00/0.0695.00/0.3995.00/0.0096.25/0.06Fault1893.75/0.5893.75/0.3293.75/0.0693.75/0.64Fault1998.75/0.13100.00/0.0098.75/0.0098.75/0.00Fault2098.75/0.3297.50/0.6498.75/0.3298.75/0.19Average87.20/0.6792.37/0.4092.55/0.3294.08/0.31IDVs1,4–7,and14arenotlistedbecausetheirFDR=1.00andFPR=0.00forallmethods.Table3FaultdiagnosisperformanceforTEprocessofvariantmodel.ModelmFDR(%)mFPR(%)Variant(a)92.250.65Variant(b)92.430.58Variant(c)92.890.60Variant(d)92.560.43Variant(e)91.530.85Variant(f)91.790.75IPO-ViT94.080.31wheremFDRandmFPRrepresenttheaverageFDRandFPRof20IDVs,respectively.K.ZhouetalProcessSafetyandEnvironmentalProtection170(2023)660–669667theglobalattention.3.2.2.TheinformationalignmentSinceCNNusesconvolutionkernelstoprocesstheoriginalinputmatrix,therewillbeamixtureofinformationcontainedinboththesampledimensionandthevariabledimension.OurproposedIPO-ViTmodelusesrectangularpatchoperation,whichisbettersuitableforindustrialprocessdata.Therefore,thereisnodoubtthatthedataoutputbyCNNdestroystheinformationalignmentintroducedbyourproposedIPO-ViT,whichmakesthedatamoreconfusingandredundantinthedimensionofvariableswhenperformingpatchembeddingoperations.Itisworthnotingthattheabovetwoproblemsarenotseriousinthefieldofcomputervision,butareespeciallyimportantinindustrialprocesses.Thisisbecauselocalsemanticsareextremelyimportantforimages.Themainbodiesandboundariesofobjectsareconstructedbylocalinformationcomparedtoentireimages,soformingalargerattentionlocallyismorehelpfulforthemodeltorecognizeobjectsinimages,soastomakebetteridentifications.Butforindustrialprocess,faultinformationcanbeemergedinslowdriftofvariablesalongonlycolumndimensioninsteadrawdimension.Therefore,identifyingsampleswithdifferenttime-lagfeatures(differentdistancesalongcolumndimension)maybeabletodiagnosefaultsmoreaccurately.Thisrequiresthemodelnottobelimitedbylocalinformation,buttoincreasetheattentionofglobalembeddingvectors.Also,rowandcolumninformationareequalforimages,butnotforindustrialprocessdata.Therefore,forimages,thereisnoseverlocalattentionexplosionproblemnorinformationalignmentproblem,butinthefieldofindustrialprocesses,itcannotbeignored.Insummary,theabovetwoproblemsmadetheperformanceofVariant(a)worsethantheoriginalglobalattentionIPO-ViTmethod.OnepossiblesolutiontoovercomethesetwoproblemswithoutcausingtoomuchmorecomputationandparameteramountsistouseaskipconnectiontoinputrawdatabothtoconvolutionlayersandTransformerblocks.Therefore,thestructureofVariant(b).1.InVariant(b),rawdatacanbeinputtothefirstTransformerblockthroughaskipconnectionchannel.TheglobalfeaturesinherentinrawdataaresupposedtoextractbythefollowingTransformerblocks,whichissupposedtoimprovetheFDDperformancetogetherwiththelocalfeaturesextractedbyconvolutionlayers.FromTable3,Variant(b)didgethighermFDRandlowermFPRcomparedwithVariant(a).Sincethedataisembeddedthroughpatches,thelocalfeaturesandglobalinformationareapproximatelylinearlysuperimposed.Therefore,thelocalattentionexplosionproblemandtheinformationalignmentproblemintheembeddingofthetransformerinputstillexists,butonlyslightlyalleviated.ThisiswhytheperformanceofVariant(b)isbetterthanthatofVariant(a),butisstillworsethantheproposedIPO-ViTmodel.Furthermore,intheabovetwovariantmodels,nosignificantperformanceimprovementwasobservedwithincreasingordecreasingthenumberoflayersofCNN.SinceVariant(b)doesnotliveuptoexpectations,consideringmovingtheinformationfusionfromthetopofthenetworktothebottomofthenetworkmayimprovetheperformanceofFDD.ThatistheideaofVariant(c).2.InVariant(c),thefeaturesoutputbytheCNNbrancharefusedwiththeinformationoutputbythetransformerbranchbeforetheclassificationlayer.BecausetheparameteramountoftheproposedIPOViTmodelis3.47Mwiththedefaultstructure(refertoTableS3).Mobilenetv2withaparameteramountof3.4MwasselectedtobalancethemainparameteramountsofCNNbranchandTransformerbranch.Inthisvariant,theeffectsofthelocalattentionexplosionproblemandtheinformationalignmentproblemareminimized,butthemajorworkasinformationfusionandclassdecisionboundarydivisionfallsontheclassificationlayer.Therefore,theclassificationperformancemainlydependsontheexpressiveabilityofit.Theoretically,theoutputsofthetransformerandCNNareusuallynotinthesamefeaturesubspace.TheclassificationlayerneedstointegratetheoutputsofbothtransformerandCNNbranchesindifferentfeaturesubspacesandtolearnagoodclassificationrepresentation.Suchrequirementonsimultaneousclassdivisionindifferentfeaturesubspacesmaydamagetheclassificationperformanceofthemodel.Therefore,themFDR(meanFDR)ofVariant(c)ishigherthanthatofVariant(a)andVariant(b),butstilllowerthanIPO-ViT.Additionally,noimprovementinmFDRwasobservedintheexperimentsofotherCNNstructureswithasimilarnumberofparameters.IfglobalfeaturesandlocalfeaturesrespectivelyextractedbyTransformerblocksandCNNblockswereintegratedmoreoften,thenlessclassificationpressuremaybeputontheclassificationlayer.ThiscomestheideaofVariant(d).3.Variant(d)progressivelyintegratesfeaturesfromtheCNNbranchandTransformerbranchlayerbylayerinsteadofmergingallfeaturesbeforetheclassificationlayer.Thisnetworkstructurereducestheneedfortheexpressivepowerofclassificationlayer,butinevitablyintroducesthelocalattentionexplosionandinformationalignmentproblems.Therefore,noobviousimprovementhasbeenmadebyitcomparedwithVariant(c).However,comparedtoVariant(a)andVariant(b),Variant(d)hasimprovedperformanceinbothmFDRandmFPRvalues.4.Variant(e)isinspiredbytheWCNN(Lietal.,2020),awavelettransformlayerhasbeenintroducedbeforetheproposedIPO-ViTmodel.TheglobalattentionmodelhasbetterrepresentationabilitythanCNN,soitcanmoreeffectivelyusehigh-frequencyfeaturestoassistlow-frequencyfeaturestocorrectlyclassfaultcategories.Thewavelettransformlayermayfilteroutnoiseinherentinsampleddata,butitmayalsoremovetoomanyhigh-frequencyfeatures,whichdegradestheperformanceoftheTransformermodel.5.Variant(f)issimilarwithVariant(e),butaskipconnectionoperationisintroduced.Thepreservationoftheoriginalinformationinrawdataissupposedtohelptoimprovethefaultidentificationresults,buttheproblemofinformationfusioninthisvariantstillexists,andtheattentionofthehigh-frequencypartmaystillbedamagedbythewavelettransform,thusaffectingthemodelingresultsoftheTransformer.ThisisthereasonthattheperformanceofVariant(f)isbetterthanVariant(e),butfarlessthanthoseofothervariantsandtheIPOViTmethod.Amulti-layerofwaveletblocksmaybecomethefutureresearchbutitbeyondthemajorconcernofourstudy,therefore,wedidn’ttryotherstructures.IttendstobeastandardchoicewhentryingtofurtherimprovetheperformanceofTransformer-basedmodels.Althoughthesevariantsmadethemodelstructuremorecomplicated,theydidnotachievemuchbetterperformanceevenafterbeingwelloptimized.ButtherearestillknowledgeandexperiencevaluableforfurtherdeeplearningstructuredesigningforFDDoranyotherindustrialprojects:thelocalattentionexplosion,theinformationalignmentandtheexpressiveabilityarethreemajorreasonslimitingtheperformanceofacomplexdeeplearningmodelwhichhaven’tbeenpaidenoughattention.1.Thelocalattentionexplosion:itmeanstheattentionofTransformerblockshasbeenwronglyattractedbythefocalfeaturesextractedbyconvolutionlayers,whichmakesrealusefulinformationhasn’tgotenoughattentionorevenbeenignored.ThisissuehappenswhencombiningconvolutionlayerswithTransformerblocks.2.Theinformationalignment:ThisisacommonissueforanynetworkstructureusingskipconnectionsorcombingtheoutputofconvolutionlayersandTransformerblocks.ButinFDD,itcanalsobeenhancedbyabackground-specificproblem.Unlikeregularimagesincomputervision,industrialdataalwayshavedifferentmeaningsandfunctionsinraworcolumndimensions.Forexample,inourcaseaswhatwementionedabove,rowsarecomposedofdifferentprocessvariables,andcolumnsarecomposedofsamplingvaluesofthesameK.ZhouetalProcessSafetyandEnvironmentalProtection170(2023)660–669668variableatdifferenttimes.Rectanglekernelsinsteadofsquarekernelsaresuggested.3.Theexpressiveabilityofclassificationlayers:MLPisusuallyusedasthelastpartofdeeplearningmodel.However,theexpressiveabilityofMLPhavebeenquestioned(Zhaoetal.,2021).Inourcase,whenthepressureofinformationfusionisconcentratedontheclassificationlayer,theexpressionabilityofMLPmaybecomeabottleneckthatlimitstheperformanceofthemodel.Justlikethebucketeffectthathowmuchwaterabucketcanholdisdeterminedbytheshorteststickinsteadofthelongestone.Foracomplexmulti-componentnetworkarchitecturemodellikeanyabovementionedvariant,althoughthecapabilityofeachcomponentitselfisalreadystrong,theperformanceoftheentirenetworkdependsnotonlyoneachcomponentofthemodel,butalsoontheoptimalcombinationofeachcomponent.Allpartsofacomplexdeeplearningnetworkworkasawholesystem.Morecomplexnetworkstructuresdonotnecessarilyguaranteebetterperformance.Howgoodtheperformanceofsuchasystemisdeterminedbytheweakestpartofitinsteadofthestrongestpart.1.ThevariantmodelswithskipconnectionshavehighermFDRthanthosevariantswithoutskipconnections.ThismaybeduetothepowerfulfeatureextractingabilityofbothtransformerandCNNbranches.Italsomaybeduetotheskipconnectioncanavoidthelocalattentionexplosionproblembyretainingtheoriginaldataandentersitintothetransformerblock.Althoughskipconnectioncannoteliminatethenegativeeffectsoflocalattentionexplosionandinformationalignmentproblems,itcaneffectivelyweakenthem.2.Optimizingembeddingmethodscanbetterfuselocalfeaturesandglobalinformation,perhapsasolutiontoreduceoreveneliminatetheimpactofinformationalignmentproblems.DevelopinganembeddingmethodmoresuitableforchemicalprocessdatamaybringnewideastosolvethefaultdiagnosismethodofCNNandtransformerfusion.3.ShallowfeatureextractionsuchaswaveletcanbasicallybereplacedbyCNNorTransformer,soitisnotrecommendedtobeusedincomplexdeeplearningframeworks.Inaddition,therearesomenewdevelopmentsinCNN,suchasadoptinglargeconvolutionkernelsonstem(Dingetal.,2022).Thesemethodshavenotbeenrigorouslyvalidatedandusuallyonlyperformwellonspecificdatasets.Consideringthestabilityandpracticalrequirementsofthemethodsappliedinthechemicalindustry,thesemethodshavenotbeencomprehensivelyevaluatedinthisstudy.Fromtheabovediscussion,itcanbeseenthatthemainproblemthatlimitsthefusionoflocalreceptivefieldsandglobalattentioninFDDisthelong-termsequencerequiredbyFDDandtheinconsistencyoftherowsandcolumnsofthedatamatrix.Duetothesetwoproblems,theindustrialdatadoesnotsatisfythepriorassumptionsoftheimage,leadingtolocalattentionexplosionandinformationalignmentproblems.Infollow-upresearch,bridgingthepriorgapbetweenindustrialdataandimagedatamaybeanimportantmethodtosolvethisproblem.Inaddition,somerecentresearchresults,suchashierarchicalViTstructure(Liuetal.,2021;Wuetal.,2021),localwindowattention(Liuetal.,2021),andtheapplicationofdepthwiseseparableconvolution(Heoetal.,2021)mayopenupnewideasforthisproblem\n",
      "40257\n"
     ]
    }
   ],
   "source": [
    "# statistic the number of characters in each string\n",
    "\n",
    "txt = open('resources\\input.txt', 'r').read()\n",
    "txt = txt.replace('\\n', '')\n",
    "txt = txt.replace('\\t', '')\n",
    "txt = txt.replace('\\r', '')\n",
    "txt = txt.replace(' ', '')\n",
    "print(txt)\n",
    "char_num = len(txt)\n",
    "print(char_num)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
