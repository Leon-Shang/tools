{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## txt to dict in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read .txt file\n",
    "import re\n",
    "import json\n",
    "input_dir = 'resources/txt/input.txt'\n",
    "file_block_delim = 'Leon424_new_message\\n'\n",
    "file_line_delim = '\\n'\n",
    "this_turn_dir = 'resources/json/this_turn.json'\n",
    "next_turn_dir = 'resources/json/next_turn.json'\n",
    "\n",
    "\n",
    "\"\"\"_txt_structure_\n",
    "Leon424_new_message\n",
    "Integrated_Generative_Model_for_Industrial_Anomaly_Detection_via_Bidirectional_LSTM_and_Attention_Mechanism.pdf\n",
    "1. Hidden Markov model (HMM)\n",
    "2. Local Outlier Factor (LOF)\n",
    "3. Generative Adversarial Network (GAN)\n",
    "4. Bidirectional Long Short-Term Memory (LSTM)\n",
    "5. Attention Mechanism (AM)\n",
    "\n",
    "Leon424_new_message\n",
    "Integrated_Generative_Model_for_Industrial_Anomaly_Detection_via_Bidirectional_LSTM_and_Attention_Mechanism.pdf\n",
    "- LSTM\n",
    "- GAN\n",
    "- Attention mechanism\n",
    "\"\"\"\n",
    "\n",
    "def delete_ordered_list_number(input_text):\n",
    "    if re.match(r'^\\d+\\. ', input_text):\n",
    "        return input_text[re.match(r'^\\d+\\. ', input_text).span()[1]:]\n",
    "    else:\n",
    "        return input_text\n",
    "\n",
    "def delete_unordered_list_symbol(input_text):\n",
    "    if re.match(r'- ', input_text):\n",
    "        return input_text[re.match(r'- ', input_text).span()[1]:]\n",
    "    else:\n",
    "        return input_text\n",
    "\n",
    "def delete_space_at_the_beginning(input_text):\n",
    "    if re.match(r'^\\s+', input_text):\n",
    "        return input_text[re.match(r'^\\s+', input_text).span()[1]:]\n",
    "    else:\n",
    "        return input_text\n",
    "\n",
    "def convert_comma_to_list(input_text):\n",
    "    return input_text.split(',')\n",
    "\n",
    "def delete_included_string_in_set(input_set):\n",
    "    original_set = input_set.copy()\n",
    "    for x in original_set:\n",
    "        for y in original_set:\n",
    "            if x in y and x != y and x in input_set:\n",
    "                input_set.remove(x)\n",
    "    return input_set\n",
    "\n",
    "def delete_same_string_lower_in_set(input_set):\n",
    "    original_set = input_set.copy()\n",
    "    delete_dict = {}\n",
    "    for x in original_set:\n",
    "        for y in original_set:\n",
    "            if x.lower() == y.lower() and x != y :\n",
    "                delete_dict[x.lower()] = x\n",
    "    for x in delete_dict.values():\n",
    "        if x in input_set:\n",
    "            input_set.remove(x)\n",
    "    return input_set\n",
    "\n",
    "def txt_to_json(input_dir, file_block_delim, file_line_delim, this_turn_dir, next_turn_dir):\n",
    "    with open(input_dir, 'r') as f:\n",
    "        input_txt = f.read()\n",
    "    input_text_list = input_txt.split(file_block_delim)[1:]\n",
    "    input_text_lists = [x.split(file_line_delim) for x in input_text_list]\n",
    "    input_text_dict = {}\n",
    "\n",
    "    for x in input_text_lists:\n",
    "        if x[0] not in input_text_dict.keys():\n",
    "            input_text_dict[x[0]] = {}\n",
    "            message_index = 0\n",
    "        else:\n",
    "            if 'for references' in x[1].lower() and 'break' in x[1].lower():\n",
    "                continue\n",
    "            message_index += 1\n",
    "        input_text_dict[x[0]][f'message_{message_index}'] = []\n",
    "        for y in x[1:]:\n",
    "            if y != '':\n",
    "                y = delete_ordered_list_number(y)\n",
    "                y = delete_unordered_list_symbol(y)\n",
    "                if ',' in y:\n",
    "                    y = convert_comma_to_list(y)\n",
    "                    \n",
    "                else:\n",
    "                    y = [y]\n",
    "                y = [delete_space_at_the_beginning(z) for z in y]\n",
    "                input_text_dict[x[0]][f'message_{message_index}'] += y\n",
    "    input_text_dict = {\n",
    "        x: {y_k: y_v for y_k, y_v in y.items() if y_k !='message_0'} for x, y in input_text_dict.items()\n",
    "    }\n",
    "    input_text_dict_set = {\n",
    "        x: list([value  for z in y.values() for value in z]) for x, y in input_text_dict.items()\n",
    "    }\n",
    "    input_text_dict_set = {\n",
    "        x: delete_same_string_lower_in_set(delete_included_string_in_set(y)) for x, y in input_text_dict_set.items()\n",
    "    }\n",
    "    input_text_dict_set_num = {\n",
    "        x: len(y) for x, y in input_text_dict_set.items()\n",
    "    }\n",
    "\n",
    "    this_turn_dict = {}\n",
    "    next_turn_dict = {}\n",
    "    for key, value in input_text_dict_set.items(): \n",
    "        if input_text_dict_set_num[key] > 10:\n",
    "            next_turn_dict[key] = value\n",
    "        else:\n",
    "            this_turn_dict[key] = value\n",
    "\n",
    "    for key, value in this_turn_dict.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "\n",
    "    for key, value in next_turn_dict.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "    with open(this_turn_dir, 'w') as f:\n",
    "        json.dump(this_turn_dict, f ,indent=4)\n",
    "\n",
    "    with open(next_turn_dir,'w') as f:\n",
    "        json.dump(next_turn_dict, f, indent= 4)\n",
    "\n",
    "txt_to_json(input_dir, file_block_delim, file_line_delim, this_turn_dir, next_turn_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## divide json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os \n",
    "\n",
    "\"\"\"_json_structure_\n",
    "{\n",
    "\"Masked_Swin_Transformer_Unet_for_Industrial_Anomaly_Detection.pdf\": {\n",
    "    \"CNN-based anomaly detection algorithms\": \"Yes.\",\n",
    "    \"CutPaste (data enhancement-based strategy)\": \"Uncertain\"\n",
    "}, \n",
    "\"Multivariate_Time-Series_Prediction_in_Industrial_Processes_via_a_Deep_Hybrid_Network_Under_Data_Uncertainty.pdf\": {\n",
    "    \"DCGNet\": \"Uncertain. Please provide more context or information.\",\n",
    "\"\"\"\n",
    "\n",
    "input_dir = 'resources/json/message_output.json'\n",
    "output_folder = 'resources/json/'\n",
    "\n",
    "def get_yes_uncertain_json(input_dir, output_folder):\n",
    "    with open(input_dir, 'r') as f:\n",
    "        result_dict = json.load(f)\n",
    "\n",
    "    yes_dict = {}\n",
    "    uncertain_dict = {}\n",
    "\n",
    "    for key, value in result_dict.items():\n",
    "        yes_dict[key] = []\n",
    "        uncertain_dict[key] = []\n",
    "        for v_k, v_v in value.items():\n",
    "            if 'yes' in v_v.lower():\n",
    "                yes_dict[key].append(v_k)\n",
    "            else: \n",
    "                uncertain_dict[key].append(v_k)\n",
    "\n",
    "    for key, value in yes_dict.items():\n",
    "        print(f'yes_{key}: {len(value)}')\n",
    "        print(f'uncertain_{key}: {len(uncertain_dict[key])}') \n",
    "\n",
    "    with open(f'{output_folder}yes_dict.json', 'w') as f:\n",
    "        json.dump(yes_dict, f, indent=4)\n",
    "\n",
    "get_yes_uncertain_json(input_dir, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import csv\n",
    "import os \n",
    "\n",
    "\n",
    "input_folder = 'resources/json/'\n",
    "json_dir_list = ['this_turn.json', 'yes_dict.json']\n",
    "output_dir = os.path.join(input_folder, 'final_output.json')\n",
    "def combine_json(input_folder, json_dir_list, output_dir):\n",
    "\n",
    "    for i, json_dir in enumerate(json_dir_list):\n",
    "        file_dir = os.path.join(input_folder, json_dir)\n",
    "        with open(file_dir, 'r') as f:\n",
    "            if i == 0:\n",
    "                result_dict = json.load(f)\n",
    "            else:\n",
    "                result_dict.update(json.load(f))\n",
    "\n",
    "    with open(output_dir, 'w') as f:\n",
    "        json.dump(result_dict, f, indent=4)\n",
    "\n",
    "combine_json(input_folder, json_dir_list, output_dir)\n",
    "\n",
    "# # write json to csv\n",
    "# with open('final_output.csv', 'w', newline='') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(['name', 'message'])\n",
    "#     for key, value in result_dict.items():\n",
    "#         writer.writerow([key, ', '.join(value)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write json to csv\n",
    "\n",
    "# import csv\n",
    "# import json\n",
    "\n",
    "# with open('final_output.json', 'r') as f:\n",
    "#     result_dict = json.load(f)\n",
    "# with open('final_output.csv', 'w', newline='') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(['name', 'message'])\n",
    "#     for key, value in result_dict.items():\n",
    "#         writer.writerow([key, ', '.join(value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "input_md_dir = 'resources/md/review.md'\n",
    "input_json_dir = 'resources/json/final_output.json'\n",
    "output_dir = 'resources/csv/final_output.csv'\n",
    "\n",
    "\"\"\"_md_structure_\n",
    "# Detection\n",
    "\n",
    "## Transfer learning\n",
    "\n",
    "Process Monitoring Using Domain-Adversarial Probabilistic Principal Component Analysis: A Transfer Learning Framework, IEEE Transactions on Industrial Informatics, 2023\n",
    "\n",
    "综述：文章提出了一种新的基于PPCA的迁移学习方法DAPPCA，利用PPCA进行特征提取，再通过logistic对提取到的特征进行所属域分类。PPCA特征提取器和域分类器之间的对抗能够使模型提取出各个域之间的共同特征，实现知识的迁移。变分推断用于训练模型，得到模型的参数。DAPPCA主要用于解决新的过程模式缺少故障数据的问题，在数值数据和工业数据上的测试结果证明了其能够提高对新过程模式的故障探测能力。\n",
    "\n",
    "模型图：![image.png](review_files\\attach_2_image.png)\n",
    "\n",
    "案例数据：\n",
    "- Simulated example\n",
    "- Electrical Submersible Pump (ESP) \n",
    "\n",
    "\n",
    "Safety Poka Yoke in Zero-Defect Manufacturing Based on Digital Twins, IEEE Transactions on Industrial Informatics, 2023\n",
    "\n",
    "综述：在这篇文章中，作者提出了基于主动学习-深度神经网络（AL-DNN）和领域对抗神经网络（DANN）的设备故障探测和诊断算法。此外，还为智能制造管理设计了一个数字孪生车间管理和控制系统。AL-DNN首先通过SDAE-based DNN以无监督的方式进行特征提取，再通过主动学习对提取到的特征进行故障探测。DANN通过域分类器和故障分类器之间的对抗提取域之间的通用特征。实验探索表明，AL-DNN算法的准确率高达99.248%，DANN的准确率可以提高20.256%，与传统算法相比具有更高的准确性。\n",
    "\n",
    "模型图：![image.png](review_files\\attach_3_image.png)\n",
    "![image-2.png](review_files\\attach_3_image-2.png)\n",
    "\n",
    "案例数据：\n",
    "- Case Western Reserve University bearing dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"paper name difference\n",
    "md: Safety Poka Yoke in Zero-Defect Manufacturing Based on Digital Twins\n",
    "json: Masked_Swin_Transformer_Unet_for_Industrial_Anomaly_Detection.pdf\n",
    "\"\"\"\n",
    "def if_delete_from_list(string):\n",
    "    if string == '':\n",
    "        return True\n",
    "    if '综述' in string:\n",
    "        return True\n",
    "    if '模型图' in string:\n",
    "        return True\n",
    "    if string.startswith('![image'):\n",
    "        return True\n",
    "    if '案例数据' in string:\n",
    "        return True\n",
    "    if string.startswith('-'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def write_models_to_csv_hierarchy(input_md_dir, input_json_dir, output_dir):\n",
    "        \n",
    "    with open(input_md_dir, 'r') as f:\n",
    "        input_text = f.read()\n",
    "    input_list = input_text.split('\\n')\n",
    "    input_list = [x for x in input_list if if_delete_from_list(x) == False]\n",
    "    output_dict = {}\n",
    "\n",
    "    for x in input_list:\n",
    "        if x.startswith('# '):\n",
    "            present_first_level = x[2:]\n",
    "            present_second_level = ''\n",
    "        elif x.startswith('## '):\n",
    "            present_second_level = x[3:]\n",
    "        else:\n",
    "            title = x.split(',')[0]\n",
    "            output_dict[title] = [present_first_level, present_second_level]\n",
    "\n",
    "    hierarchy_dict = {}\n",
    "    for key, value in output_dict.items():\n",
    "        if value[0] not in hierarchy_dict.keys():\n",
    "            hierarchy_dict[value[0]] = {}\n",
    "        if value[1] not in hierarchy_dict[value[0]].keys():\n",
    "            hierarchy_dict[value[0]][value[1]] = []\n",
    "        hierarchy_dict[value[0]][value[1]].append(key)\n",
    "\n",
    "    with open(input_json_dir, 'r') as f:\n",
    "        model_name_dict = json.load(f)\n",
    "\n",
    "    with open(output_dir, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for key, value in hierarchy_dict.items():\n",
    "            writer.writerow([key, 'relevant model'])\n",
    "            for paper_list in value.values():\n",
    "                for paper in paper_list:\n",
    "                    try:\n",
    "                        writer.writerow([paper, ', '.join(model_name_dict[paper.replace(' ', '_')+'.pdf'])])\n",
    "                    except:\n",
    "                        print(paper)\n",
    "\n",
    "write_models_to_csv_hierarchy(input_md_dir, input_json_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
